{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rows = []\n",
    "doc = nlp(u\"Steve Jobs and Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    rows.append((token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop))\n",
    "    \n",
    "data = pd.DataFrame(rows, columns=[\"text\", \"lemma\", \"part_of_speech\", \"tag\", \"dependency\", \"shape\", \"is_alphanumeric\", \"is_stopword\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steve Jobs 0 10 PERSON\n",
      "Apple 15 20 ORG\n",
      "U.K. 42 46 GPE\n",
      "$1 billion 59 69 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Steve Jobs and Apple is looking at buying U.K. startup for $1 billion\")\n",
    "import en_core_web_sm, en_core_web_md\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize this using displacy:\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings (word2vec Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words (Use Context to Predict Target Word)\n",
    "![alt text](images/word2vec_cbow.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "![alt text](images/softmax.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skipgram is an iterative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram\n",
    "![alt text](images/skipgram.png \"Logo Title Text 1\")\n",
    "\n",
    "## Softmax\n",
    "![alt text](images/wordembedding_cluster.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dog - cat: 0.44096532464027405\n",
      " dog - Beijing: 0.1896352618932724\n",
      " dog - sad: 0.2974436283111572\n",
      " dog - depressed: 0.09678884595632553\n",
      " dog - couch: 0.3066478967666626\n",
      " dog - sofa: 0.36296597123146057\n",
      " dog - canine: 0.29539573192596436\n",
      " dog - China: 0.12491319328546524\n",
      " dog - Chinese: 0.05871826782822609\n",
      " dog - France: 0.23890900611877441\n",
      " dog - Paris: 0.2118028998374939\n",
      " dog - banana: 0.3797188103199005\n",
      " cat - dog: 0.44096532464027405\n",
      " cat - Beijing: 0.37668392062187195\n",
      " cat - sad: 0.363819420337677\n",
      " cat - depressed: 0.09524044394493103\n",
      " cat - couch: 0.36932238936424255\n",
      " cat - sofa: 0.39886417984962463\n",
      " cat - canine: 0.2745530605316162\n",
      " cat - China: 0.36744892597198486\n",
      " cat - Chinese: 0.05915885046124458\n",
      " cat - France: 0.3288818597793579\n",
      " cat - Paris: 0.25500184297561646\n",
      " cat - banana: 0.2804490923881531\n",
      " Beijing - dog: 0.1896352618932724\n",
      " Beijing - cat: 0.37668392062187195\n",
      " Beijing - sad: 0.027374835684895515\n",
      " Beijing - depressed: 0.091560959815979\n",
      " Beijing - couch: 0.17345663905143738\n",
      " Beijing - sofa: 0.30311763286590576\n",
      " Beijing - canine: 0.28667938709259033\n",
      " Beijing - China: 0.6202490329742432\n",
      " Beijing - Chinese: 0.2613280713558197\n",
      " Beijing - France: 0.5173807740211487\n",
      " Beijing - Paris: 0.3785735070705414\n",
      " Beijing - banana: 0.23265081644058228\n",
      " sad - dog: 0.2974436283111572\n",
      " sad - cat: 0.363819420337677\n",
      " sad - Beijing: 0.027374835684895515\n",
      " sad - depressed: 0.4614866375923157\n",
      " sad - couch: 0.28866133093833923\n",
      " sad - sofa: 0.20324121415615082\n",
      " sad - canine: 0.07975590974092484\n",
      " sad - China: 0.055475931614637375\n",
      " sad - Chinese: 0.15277521312236786\n",
      " sad - France: 0.11607188731431961\n",
      " sad - Paris: 0.06069101020693779\n",
      " sad - banana: 0.14701955020427704\n",
      " depressed - dog: 0.09678884595632553\n",
      " depressed - cat: 0.09524044394493103\n",
      " depressed - Beijing: 0.091560959815979\n",
      " depressed - sad: 0.4614866375923157\n",
      " depressed - couch: 0.31701767444610596\n",
      " depressed - sofa: 0.2198280394077301\n",
      " depressed - canine: 0.04722786322236061\n",
      " depressed - China: 0.10068430006504059\n",
      " depressed - Chinese: 0.2706618309020996\n",
      " depressed - France: 0.19519655406475067\n",
      " depressed - Paris: 0.03223919868469238\n",
      " depressed - banana: 0.12356306612491608\n",
      " couch - dog: 0.3066478967666626\n",
      " couch - cat: 0.36932238936424255\n",
      " couch - Beijing: 0.17345663905143738\n",
      " couch - sad: 0.28866133093833923\n",
      " couch - depressed: 0.31701767444610596\n",
      " couch - sofa: 0.38562875986099243\n",
      " couch - canine: 0.3643229007720947\n",
      " couch - China: 0.23057174682617188\n",
      " couch - Chinese: 0.10992041975259781\n",
      " couch - France: 0.11669176816940308\n",
      " couch - Paris: 0.026960982009768486\n",
      " couch - banana: 0.2278093695640564\n",
      " sofa - dog: 0.36296597123146057\n",
      " sofa - cat: 0.39886417984962463\n",
      " sofa - Beijing: 0.30311763286590576\n",
      " sofa - sad: 0.20324121415615082\n",
      " sofa - depressed: 0.2198280394077301\n",
      " sofa - couch: 0.38562875986099243\n",
      " sofa - canine: 0.3848804533481598\n",
      " sofa - China: 0.3007420301437378\n",
      " sofa - Chinese: 0.2411765456199646\n",
      " sofa - France: 0.3376525342464447\n",
      " sofa - Paris: 0.2623986303806305\n",
      " sofa - banana: 0.2799041271209717\n",
      " canine - dog: 0.29539573192596436\n",
      " canine - cat: 0.2745530605316162\n",
      " canine - Beijing: 0.28667938709259033\n",
      " canine - sad: 0.07975590974092484\n",
      " canine - depressed: 0.04722786322236061\n",
      " canine - couch: 0.3643229007720947\n",
      " canine - sofa: 0.3848804533481598\n",
      " canine - China: 0.38103508949279785\n",
      " canine - Chinese: 0.2516288757324219\n",
      " canine - France: 0.355733186006546\n",
      " canine - Paris: 0.30544620752334595\n",
      " canine - banana: 0.39167332649230957\n",
      " China - dog: 0.12491319328546524\n",
      " China - cat: 0.36744892597198486\n",
      " China - Beijing: 0.6202490329742432\n",
      " China - sad: 0.055475931614637375\n",
      " China - depressed: 0.10068430006504059\n",
      " China - couch: 0.23057174682617188\n",
      " China - sofa: 0.3007420301437378\n",
      " China - canine: 0.38103508949279785\n",
      " China - Chinese: 0.36784279346466064\n",
      " China - France: 0.6704185605049133\n",
      " China - Paris: 0.45149847865104675\n",
      " China - banana: 0.2742655575275421\n",
      " Chinese - dog: 0.05871826782822609\n",
      " Chinese - cat: 0.05915885046124458\n",
      " Chinese - Beijing: 0.2613280713558197\n",
      " Chinese - sad: 0.15277521312236786\n",
      " Chinese - depressed: 0.2706618309020996\n",
      " Chinese - couch: 0.10992041975259781\n",
      " Chinese - sofa: 0.2411765456199646\n",
      " Chinese - canine: 0.2516288757324219\n",
      " Chinese - China: 0.36784279346466064\n",
      " Chinese - France: 0.5611129999160767\n",
      " Chinese - Paris: 0.4804312586784363\n",
      " Chinese - banana: 0.25190404057502747\n",
      " France - dog: 0.23890900611877441\n",
      " France - cat: 0.3288818597793579\n",
      " France - Beijing: 0.5173807740211487\n",
      " France - sad: 0.11607188731431961\n",
      " France - depressed: 0.19519655406475067\n",
      " France - couch: 0.11669176816940308\n",
      " France - sofa: 0.3376525342464447\n",
      " France - canine: 0.355733186006546\n",
      " France - China: 0.6704185605049133\n",
      " France - Chinese: 0.5611129999160767\n",
      " France - Paris: 0.6466256976127625\n",
      " France - banana: 0.3096224069595337\n",
      " Paris - dog: 0.2118028998374939\n",
      " Paris - cat: 0.25500184297561646\n",
      " Paris - Beijing: 0.3785735070705414\n",
      " Paris - sad: 0.06069101020693779\n",
      " Paris - depressed: 0.03223919868469238\n",
      " Paris - couch: 0.026960982009768486\n",
      " Paris - sofa: 0.2623986303806305\n",
      " Paris - canine: 0.30544620752334595\n",
      " Paris - China: 0.45149847865104675\n",
      " Paris - Chinese: 0.4804312586784363\n",
      " Paris - France: 0.6466256976127625\n",
      " Paris - banana: 0.3281705379486084\n",
      " banana - dog: 0.3797188103199005\n",
      " banana - cat: 0.2804490923881531\n",
      " banana - Beijing: 0.23265081644058228\n",
      " banana - sad: 0.14701955020427704\n",
      " banana - depressed: 0.12356306612491608\n",
      " banana - couch: 0.2278093695640564\n",
      " banana - sofa: 0.2799041271209717\n",
      " banana - canine: 0.39167332649230957\n",
      " banana - China: 0.2742655575275421\n",
      " banana - Chinese: 0.25190404057502747\n",
      " banana - France: 0.3096224069595337\n",
      " banana - Paris: 0.3281705379486084\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'dog cat Beijing sad depressed couch sofa canine China Chinese France Paris banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        if token1 != token2:\n",
    "            print(f\" {token1} - {token2}: {1 - cosine(token1.vector, token2.vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Most Similar Words (Using Our Old Methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# inspect the default settings for CountVectorizer\n",
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = open(\"poor_amazon_toy_reviews.txt\").readlines()\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=500,token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b')\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "data = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create similiarity matrix\n",
    "similarity_matrix = pd.DataFrame(cosine_similarity(data.T.values), \n",
    "             columns=vectorizer.get_feature_names(),\n",
    "                                 index=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack matrix into table\n",
    "similarity_table = similarity_matrix.rename_axis(None).rename_axis(None, axis=1).stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "similarity_table.columns = [\"word1\", \"word2\", \"similarity\"]\n",
    "similarity_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_table = similarity_table[similarity_table[\"similarity\"] < 0.99]\n",
    "similarity_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_table.sort_values(by=\"similarity\", ascending=False).drop_duplicates(\n",
    "    subset=\"similarity\", keep=\"first\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_500_words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Similar Words Using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into spacy your top 500 words\n",
    "\n",
    "tokens = nlp(f'{\" \".join(top_500_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# create a list of similarity tuples\n",
    "\n",
    "similarity_tuples = []\n",
    "\n",
    "for token1, token2 in product(tokens, repeat=2):\n",
    "    similarity_tuples.append((token1, token2, token1.similarity(token2)))\n",
    "\n",
    "similarities = pd.DataFrame(similarity_tuples, columns=[\"word1\",\"word2\", \"score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find similar words\n",
    "similarities[similarities[\"score\"] < 1].sort_values(\n",
    "    by=\"score\", ascending=False).drop_duplicates(\n",
    "    subset=\"score\", keep=\"first\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Most Similar Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vectors for each review\n",
    "review_vectors = []\n",
    "NUM_REVIEWS = 400\n",
    "for review in reviews[:NUM_REVIEWS]:\n",
    "    sentence = nlp(review)\n",
    "    review_vectors.append(sentence.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df =pd.DataFrame(review_vectors)\n",
    "vector_df[\"text\"] = reviews[:NUM_REVIEWS]\n",
    "\n",
    "\n",
    "vector_df.set_index(\"text\", inplace=True)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = pd.DataFrame(cosine_similarity(vector_df.values), columns=reviews[:NUM_REVIEWS], index=reviews[:NUM_REVIEWS])\n",
    "\n",
    "top_similarities = similarities.unstack().reset_index()\n",
    "top_similarities.columns = [\"review1\", \"review2\", \"similarity\"]\n",
    "top_similarities = top_similarities.sort_values(by=\"similarity\", ascending=False)\n",
    "top_similarities = top_similarities[top_similarities[\"similarity\"] < .9999].head(10)\n",
    "\n",
    "\n",
    "for idx, row in top_similarities.iterrows():\n",
    "    print(row[\"review1\"])\n",
    "    print(row[\"review2\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "vector = nlp(u'banana').vector\n",
    "\n",
    "ax = sns.distplot(vector, kde=False, rug=True)\n",
    "t = ax.set_title('Histogram of Feature Values')\n",
    "\n",
    "# most word2vec embeddings are normally distributed, sanity check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Techniques\n",
    "\n",
    "### Subsampling\n",
    "\n",
    "What do we do with highly frequent words like `the` or `of`? We don't gain a ton of meaning from training on these words, and they become computationally expensive since they appear so frequently:\n",
    "\n",
    "![alt text](images/subsampling.png \"http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\")\n",
    "In the image above, $z(w_i)$ is the frequency of that particular word divided by the total number of words in the entire corpus. For instance, if a corpus of text has 50 words, and the word `dog` appears 3 times, $z(w_{dog}) = 0.06$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# write subsampling function\n",
    "def subsample(z):\n",
    "    return ((z * 1000) ** 0.5 + 1) * (0.001 / z)\n",
    "\n",
    "# plot this function:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Z = list(np.linspace(0,1,100))\n",
    "probability_of_keeping = list(map( lambda z: subsample(z), Z))\n",
    "\n",
    "plt.scatter(Z, probability_of_keeping)\n",
    "plt.xlabel(\"Frequency word appears in corpus\")\n",
    "plt.ylabel(\"Probability of keeping\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Word Embeddings\n",
    "\n",
    "#### How to handle **Out Of Vocabulary (OOV)** words?\n",
    "Although **word2vec** and **FastText** include a significant vocabulary size, there will inevitably be words that are not included. For instance, if you are analyzing text conversations using word embeddings pretrained on Wikipedia text (which typically has more formal vocabulary than everyday language), how will you account for the following words?\n",
    "\n",
    "- DM\n",
    "- ROFLMAO\n",
    "- bae\n",
    "- 😃\n",
    "- #10YearChallenge\n",
    "- wut\n",
    "\n",
    "#### Potential solution: use word embeddings if they are available, and otherwise initialize the weights to random.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "def vectorize_word(input_word: str, D=50):\n",
    "    \"\"\"\n",
    "    D: an integer that represents the length (dimensionality of the word embeddings)\n",
    "    word_embeddings: A dictionary object with the string word as the key, and the embedding vector of \n",
    "    length D as the values.\n",
    "    For instance, word_embeddings[\"cat\"] will return [2.3, 4.5, 6.1, -2.2, ...]\n",
    "    \"\"\"\n",
    "    if input_word in word_embeddings.keys():\n",
    "        return word_embeddings[input_word]\n",
    "    else:\n",
    "        return np.random.rand(D)\n",
    "```\n",
    "\n",
    "##### Should we update the word embedding matrices during the model training step?\n",
    "- Ideally, you'd only want to be able to update the specific weights that were randomly initialized (since the rest of the weights are by definition pre-trained and are already pretty good). However, most deep learning libraries do not allow you to easily select which specific weight elements to apply backpropagation to- you either update all weights or you update none. In practice, most data scientists will \"freeze\" the word embedding layer:\n",
    "\n",
    "In Keras:\n",
    "```python\n",
    "word_embedding_layer.trainable = False # by default, trainable is set to true in Keras\n",
    "```\n",
    "In Tensorflow:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "N = 300 # number of words\n",
    "D = 50 # of dimensions in embeddings\n",
    "initial_word_embeddings = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "tensor = tf.constant(initial_word_embeddings, shape=[N, D])\n",
    "```\n",
    "\n",
    "- Ambiguity around **Domain-specific words**: using a generic pre-trained word embedding will not capture the semantic meaning of the word **sack** when it is used in the context of American football:\n",
    "![sack](images/football-bag-sack-diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://radimrehurek.com/gensim/models/word2vec.html\n",
    "from gensim.test.utils import common_texts, get_tmpfile # get_tmpfile: save a model for us\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "# pass in a list of lists\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence'],\n",
    "            [\"first\", \"and\", \"second\", \"sentence\"]] \n",
    "# train model\n",
    "# you can also specify an alpha, which is a hyperparameter learning rate\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary1\n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.5092266e-05,  1.9522295e-03,  1.4109060e-03, -1.0363804e-03,\n",
       "        9.1018976e-04, -1.6978879e-03, -1.7306680e-03,  4.7595566e-03,\n",
       "        5.9507601e-04, -2.0301137e-03, -3.1543707e-03, -1.2550931e-04,\n",
       "        4.1557365e-04,  7.8584079e-04,  2.2424064e-03, -4.2699967e-03,\n",
       "        2.2679116e-04, -2.6429801e-03,  3.4011710e-03,  3.7786164e-03,\n",
       "        1.5968040e-03, -2.1087071e-03, -2.4564702e-03,  4.7298153e-03,\n",
       "       -6.8761983e-05,  4.8470413e-03,  3.6488401e-03, -3.8178321e-03,\n",
       "       -3.4434139e-03, -1.7051263e-04,  3.6443173e-04, -1.3256480e-03,\n",
       "        1.1629105e-04, -3.4973472e-03,  3.8749522e-03, -1.8761534e-03,\n",
       "        3.3677698e-03, -3.9364309e-03, -3.6525205e-03,  7.3286262e-04,\n",
       "       -4.1624205e-03,  6.8051839e-04,  2.4304332e-03, -3.9490922e-03,\n",
       "        3.9610662e-03, -2.8323629e-03, -1.4331616e-03, -4.4142830e-04,\n",
       "       -3.9973161e-03, -4.0987930e-03,  2.9062559e-03, -4.2559486e-03,\n",
       "       -3.8320315e-03, -4.6142866e-03,  1.3906345e-03, -6.3733663e-04,\n",
       "        2.6225403e-03, -4.1410592e-03, -7.8843476e-04,  2.6762348e-03,\n",
       "       -3.4038092e-03, -3.0383747e-03,  6.0581317e-04, -4.8490791e-03,\n",
       "       -3.6586211e-03,  3.0072294e-03, -2.6934268e-03, -3.8286662e-04,\n",
       "       -4.7549079e-03,  1.2159892e-03, -2.6494437e-03, -4.6950495e-03,\n",
       "        2.4977932e-03,  2.8069133e-03,  3.0032201e-03,  1.4882329e-03,\n",
       "       -4.3355348e-03, -1.1618403e-03, -4.4585387e-03, -4.8019080e-03,\n",
       "        1.7333169e-03, -2.3965333e-03, -4.7545494e-03,  4.3909447e-03,\n",
       "       -1.7329301e-03, -6.7807647e-04,  2.1545712e-03, -4.1599255e-03,\n",
       "       -3.3842053e-03,  2.4966039e-03,  3.3681295e-04, -1.2687541e-03,\n",
       "        1.7912755e-03, -3.9467716e-04,  2.9214020e-03, -1.3608569e-03,\n",
       "        1.7818459e-03,  4.9964418e-03,  2.7917328e-03,  3.8974953e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.get_vector(\"second\") # vector that represented the word \"second\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Your Own Word2Vec Embeddings Using Gensim\n",
    "training to predict context word bases on targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = open(\"good_amazon_toy_reviews.txt\").readlines() + open(\"poor_amazon_toy_reviews.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word_tokenize(review) for review in reviews] # get a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(docs, min_count=5) # token appears at least 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Amazon', 0.8654698729515076),\n",
       " ('sale', 0.7671324014663696),\n",
       " ('site', 0.660471498966217),\n",
       " ('Amazon.com', 0.6390148401260376),\n",
       " ('Japan', 0.6285412311553955),\n",
       " ('eBay', 0.628023624420166),\n",
       " ('Ebay', 0.6125444173812866),\n",
       " ('ebay', 0.6081639528274536),\n",
       " ('Walmart', 0.6078121066093445),\n",
       " ('Target', 0.5998607277870178)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"amazon\") # getting similar words for amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GoogleNews word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ac2c30c06003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# word analogies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     )\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "# load in the entire Google News word embedding vectors\n",
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "# word analogies\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most similar words for a target word\n",
    "model.most_similar(\"cappucino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use?\n",
    "\n",
    "- traditionally, each individual word is trained onto a new word embedding\n",
    "- in many languages (including English), many words are morphologically derivative from each other. \n",
    "- use case when your corpus contains high-value, morphologically diverse, rare words (`photosynthesis`, `transcendentalism`)\n",
    "- may also be effective when your text contains lots of misspellings or abbreviations (ie. SMS, digital conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.train_unsupervised('good_amazon_toy_reviews.txt', model='skipgram', lr=0.05, dim=100, ws=5, epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(model[\"adore\"].reshape(1,-1), model[\"love\"].reshape(1,-1))\n",
    "# cosine similarity scores is between 0 and 1 only when based on word counts, tf-idf etc.\n",
    "# if based on negative values, need to normalize to get similarity score in that range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Hyperparameters (From [Tutorial Notebook](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb))\n",
    "- **model**: Training architecture. Allowed values: `cbow`, `skipgram` (Default `cbow`)\n",
    "- **size**: Size of embeddings to be learnt (Default 100)\n",
    "- **alpha**: Initial learning rate (Default 0.025)\n",
    "- **window**: Context window size (Default 5)\n",
    "- **min_count**: Ignore words with number of occurrences below this (Default 5)\n",
    "- **loss**: Training objective. Allowed values: `ns`, `hs`, `softmax` (Default `ns`)\n",
    "- **sample**: Threshold for downsampling higher-frequency words (Default 0.001)\n",
    "- **negative**: Number of negative words to sample, for `ns` (Default 5)\n",
    "- **iter**: Number of epochs (Default 5)\n",
    "- **sorted_vocab**: Sort vocab by descending frequency (Default 1)\n",
    "- **threads**: Number of threads to use (Default 12)\n",
    "\n",
    "Hyperparameters unique to `fasttext`:\n",
    "- **min_n**: min length of char ngrams (Default 3)\n",
    "- **max_n**: max length of char ngrams (Default 6)\n",
    "- **bucket**: number of buckets used for hashing ngrams (Default 2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Implementation of FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "text = list(pd.read_csv(\"bbc-text.csv\")[\"text\"].values)\n",
    "\n",
    "new_text = [word_tokenize(story) for story in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(size=200, window=4, min_count=2)  # change the size of the windows\n",
    "model.build_vocab(sentences=new_text)\n",
    "model.train(sentences=new_text, total_examples=len(new_text), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get corpus total count\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35548925, -0.2392853 ,  0.1783668 ,  0.2178757 , -0.34836483,\n",
       "       -0.13850698, -0.04159029, -0.11388176,  0.33893082,  0.07921807,\n",
       "       -0.45206115,  0.2871333 ,  0.36816663, -0.7687537 , -0.08989591,\n",
       "        0.16278489,  0.11419601, -0.12533134,  0.43718037, -0.5201533 ,\n",
       "        0.25022006, -0.42382377,  0.42026442, -0.5679746 ,  0.0815757 ,\n",
       "        0.5062104 , -0.10711527,  0.24420968, -0.439406  ,  0.60453254,\n",
       "       -0.20767434,  0.618615  , -0.56026024,  0.18472664,  0.22696745,\n",
       "       -0.09939901, -0.63494384,  0.12793192, -0.09048668, -0.06641864,\n",
       "        1.3101934 ,  0.06419657, -0.17219837, -0.24614467,  0.21924664,\n",
       "       -0.8551644 ,  0.28719473,  0.12382978,  0.18622756,  0.34361595,\n",
       "        0.00479034, -0.4991639 ,  0.3079553 , -0.29209128, -0.63230735,\n",
       "        0.4234859 , -0.15701935, -0.09134828,  0.71122354, -0.16012025,\n",
       "       -0.13554446, -0.50125176, -0.36299777, -0.2540749 ,  0.09812494,\n",
       "        0.43265495,  0.27067137,  0.61005896, -0.23883493,  0.3309526 ,\n",
       "        0.44212478, -0.12978652,  0.7196551 ,  0.17828849, -0.8421746 ,\n",
       "       -0.16766517,  0.22789986,  0.0369002 , -0.04673982,  0.00707936,\n",
       "       -0.10131461,  0.7539944 , -0.48123083, -0.3656117 , -0.12794614,\n",
       "        0.15327713,  0.19587906,  0.19041495, -0.57423496,  0.26499572,\n",
       "        0.45423335, -0.13307211,  0.03146664,  0.69019216,  0.12666164,\n",
       "        0.29372898, -0.04004633, -0.26653847, -0.32477352, -0.02034559,\n",
       "        0.12467504,  0.16604654,  0.01448037, -0.15105481, -0.33609843,\n",
       "        0.38150862, -0.16538382, -0.05303964, -0.08692597,  0.11488448,\n",
       "        0.04198315, -0.366999  ,  0.00755515,  0.26692197, -0.68321365,\n",
       "       -0.98686844,  0.21165344,  0.31631824, -0.01879602, -0.5265779 ,\n",
       "       -0.22363234, -0.847405  , -0.68228227, -1.1078789 ,  0.01023278,\n",
       "        0.267701  ,  0.08026516,  0.84628725, -0.24759007, -0.334802  ,\n",
       "       -0.57818455,  0.28511894,  0.50748277, -0.48449454, -0.33667523,\n",
       "        1.0119492 ,  0.7037978 , -0.12901272, -0.53457475, -0.35566673,\n",
       "       -0.07906926,  0.9131188 , -0.42309704, -0.2458072 , -0.25172356,\n",
       "       -0.21084842, -0.20328806, -0.21519972, -0.27185676,  0.60044914,\n",
       "       -0.3349808 , -0.15070832,  0.5565345 , -0.41642913, -0.5778182 ,\n",
       "       -0.1159066 ,  0.17895712,  0.27812204, -0.13328338,  0.00448367,\n",
       "       -0.505009  , -0.14563073,  0.6401662 ,  0.02292973, -0.24689485,\n",
       "       -0.769056  ,  0.8641531 , -0.07181908, -0.18032245,  0.30904308,\n",
       "        0.14790809, -0.8764024 ,  0.29929397, -0.66395336,  0.22158335,\n",
       "        0.28475818,  0.2268537 ,  0.35194   ,  0.15390737, -0.09354156,\n",
       "       -0.57019395, -0.25523564, -0.44303966,  0.34455824, -0.3779809 ,\n",
       "       -0.01102882, -0.41858438, -0.19602256, -0.20766728,  0.64423656,\n",
       "        0.2574719 , -0.08298393,  0.74618614,  0.398564  , -0.46736008,\n",
       "       -0.05769171, -0.2849226 , -0.53201777, -0.02882292, -0.40295354],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word vector for dog\n",
    "model.wv[\"dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get length of word embeddings\n",
    "len(model[\"king\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n",
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('transmit', 0.8640565872192383),\n",
       " ('translate', 0.8462340831756592),\n",
       " ('high-tech', 0.8430594205856323),\n",
       " ('tata', 0.8419097661972046),\n",
       " ('tech', 0.8399066925048828),\n",
       " ('data', 0.8395676612854004),\n",
       " ('connectivity', 0.8377481698989868),\n",
       " ('storage', 0.8365424275398254),\n",
       " ('transform', 0.8280481696128845),\n",
       " ('metadata', 0.8267114162445068)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"france\")\n",
    "model.most_similar(\"dog\")\n",
    "model.most_similar(\"transc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in document matrix (D), initially just random values, but added in classification work flow, so part of the loss function. not tied to any single word or context word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Memory Version of Paragraph Vector (PV-DM)\n",
    "![](images/doc2vec.png)\n",
    "\n",
    "### Distributed Bag of Words of Paragraph Vector (PV-DBOW)\n",
    "![](images/doc2vec2.png)\n",
    "[A Gentle Introduction to Doc2Vec](https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews)]\n",
    "model = Doc2Vec(documents, vector_size=50, window=4, min_count=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_vector = model.infer_vector([\"The\", \"toy\", \"was\", \"broken\", \"quickly\"]).reshape(1, -1)\n",
    "doc2_vector = model.infer_vector([\"It\", \"broke\", \"fast\"]).reshape(1, -1)\n",
    "doc3_vector = model.infer_vector([\"I ate lunch late\"]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0842022e-19]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc1_vector, doc2_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0842022e-19]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc1_vector, doc3_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc2_vector, doc3_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
