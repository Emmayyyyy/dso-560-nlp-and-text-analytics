{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are provided with two files of pre-labeled data. The labels are created by industry experts and details the tags associated with certain online fashion items. Using these pre-labeled products as training data, we can train the model to understand characteristics associated with the size of each product and predict the size label of new products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the two pre-labeled tagged data sets.\n",
    "\n",
    "df1 = pd.read_csv(\"tagged_product_attribute.csv\")\n",
    "df2 = pd.read_csv(\"usc_additional_tags.csv\")\n",
    "df = pd.concat([df1, df2], ignore_index = True)\n",
    "df_size = df[df.attribute_name == \"sizing\"].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use regex to unify the spellings on the categories\n",
    "\n",
    "df_size.attribute_value = [re.sub(r'(Regular)', 'regular', i) for i in df_size.attribute_value]\n",
    "df_size.attribute_value = [re.sub(r'(Plus)', 'plus', i) for i in df_size.attribute_value] \n",
    "df_size.attribute_value = [re.sub(r'(Maternity)', 'maternity', i) for i in df_size.attribute_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_size = df_size.drop([\"product_color_id\", \"attribute_name\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>attribute_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01E2M39XZ7JKEDP7AG4N0DK2N7</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01DPGVF16NCKCEGM24V3RQ7JZG</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01DT0DK7AV6D33GRSR0SFXDH24</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01DS1BTV804QFC91G3R9Q7D792</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01DTJCD2QG8RKQ4HJKT4GBW888</td>\n",
       "      <td>regular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id attribute_value\n",
       "0  01E2M39XZ7JKEDP7AG4N0DK2N7         regular\n",
       "1  01DPGVF16NCKCEGM24V3RQ7JZG         regular\n",
       "2  01DT0DK7AV6D33GRSR0SFXDH24         regular\n",
       "3  01DS1BTV804QFC91G3R9Q7D792         regular\n",
       "4  01DTJCD2QG8RKQ4HJKT4GBW888         regular"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_size.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with product data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the characteristics, we need to get text descriptions on the product brand, category, etc. We are provided with two datasets of products. First, we join the two datasets to get one master data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"full_data.csv\")\n",
    "data2 = pd.read_csv(\"extra_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data1[[\"product_id\",\"brand\",\"product_full_name\",\"description\",\"brand_category\",\"brand_canonical_url\",\"details\"]]\n",
    "data1=data1.rename(columns = {\"product_full_name\":\"name\"})\n",
    "data2=data2[[\"product_id\",\"brand\",\"name\",\"description\",\"brand_category\",\"brand_canonical_url\",\"details\"]]\n",
    "data = pd.concat([data1, data2], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"details\"] = data['details'].str.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop duplicate entries\n",
    "data = data.drop_duplicates(subset=\"product_id\", keep='last', inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we merge the data set with the df_size table which contains the size labels for certain products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge\n",
    "\n",
    "my_df = data.merge(df_size, how = \"outer\", left_on = \"product_id\", right_on = \"product_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop duplicate entries\n",
    "my_df = my_df.drop_duplicates(subset=\"product_id\", keep='last', inplace=False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df=my_df.rename(columns = {\"attribute_value\":\"size\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>brand_canonical_url</th>\n",
       "      <th>details</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01DSE9TC2DQXDG6GWKW9NMJ416</td>\n",
       "      <td>Banana Republic</td>\n",
       "      <td>Ankle-Strap Pump</td>\n",
       "      <td>A modern pump, in a rounded silhouette with an...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>https://bananarepublic.gap.com/browse/product....</td>\n",
       "      <td>A modern pump, in a rounded silhouette with an...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01DSE9SKM19XNA6SJP36JZC065</td>\n",
       "      <td>Banana Republic</td>\n",
       "      <td>Petite Tie-Neck Top</td>\n",
       "      <td>Dress it down with jeans and sneakers or dress...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>https://bananarepublic.gap.com/browse/product....</td>\n",
       "      <td>Dress it down with jeans and sneakers or dress...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01DSJX8GD4DSAP76SPR85HRCMN</td>\n",
       "      <td>Loewe</td>\n",
       "      <td>52MM Padded Leather Round Sunglasses</td>\n",
       "      <td>Padded leather covers classic round sunglasses.</td>\n",
       "      <td>JewelryAccessories/SunglassesReaders/RoundOval...</td>\n",
       "      <td>https://www.saksfifthavenue.com/loewe-52mm-pad...</td>\n",
       "      <td>100% UV protection Case and cleaning cloth inc...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01DSJVKJNS6F4KQ1QM6YYK9AW2</td>\n",
       "      <td>Converse</td>\n",
       "      <td>Baby's &amp; Little Kid's All-Star Two-Tone Mid-To...</td>\n",
       "      <td>The iconic mid-top design gets an added dose o...</td>\n",
       "      <td>JustKids/Shoes/Baby024Months/BabyGirl,JustKids...</td>\n",
       "      <td>https://www.saksfifthavenue.com/converse-babys...</td>\n",
       "      <td>Canvas upper Round toe Lace-up vamp SmartFOAM ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01DSK15ZD4D5A0QXA8NSD25YXE</td>\n",
       "      <td>Alexander McQueen</td>\n",
       "      <td>64MM Rimless Sunglasses</td>\n",
       "      <td>Hexagonal shades offer a rimless view with int...</td>\n",
       "      <td>JewelryAccessories/SunglassesReaders/RoundOval</td>\n",
       "      <td>https://www.saksfifthavenue.com/alexander-mcqu...</td>\n",
       "      <td>100% UV protection Gradient lenses Adjustable ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id              brand  \\\n",
       "0  01DSE9TC2DQXDG6GWKW9NMJ416    Banana Republic   \n",
       "1  01DSE9SKM19XNA6SJP36JZC065    Banana Republic   \n",
       "2  01DSJX8GD4DSAP76SPR85HRCMN              Loewe   \n",
       "3  01DSJVKJNS6F4KQ1QM6YYK9AW2           Converse   \n",
       "4  01DSK15ZD4D5A0QXA8NSD25YXE  Alexander McQueen   \n",
       "\n",
       "                                                name  \\\n",
       "0                                   Ankle-Strap Pump   \n",
       "1                                Petite Tie-Neck Top   \n",
       "2               52MM Padded Leather Round Sunglasses   \n",
       "3  Baby's & Little Kid's All-Star Two-Tone Mid-To...   \n",
       "4                            64MM Rimless Sunglasses   \n",
       "\n",
       "                                         description  \\\n",
       "0  A modern pump, in a rounded silhouette with an...   \n",
       "1  Dress it down with jeans and sneakers or dress...   \n",
       "2    Padded leather covers classic round sunglasses.   \n",
       "3  The iconic mid-top design gets an added dose o...   \n",
       "4  Hexagonal shades offer a rimless view with int...   \n",
       "\n",
       "                                      brand_category  \\\n",
       "0                                            Unknown   \n",
       "1                                            Unknown   \n",
       "2  JewelryAccessories/SunglassesReaders/RoundOval...   \n",
       "3  JustKids/Shoes/Baby024Months/BabyGirl,JustKids...   \n",
       "4     JewelryAccessories/SunglassesReaders/RoundOval   \n",
       "\n",
       "                                 brand_canonical_url  \\\n",
       "0  https://bananarepublic.gap.com/browse/product....   \n",
       "1  https://bananarepublic.gap.com/browse/product....   \n",
       "2  https://www.saksfifthavenue.com/loewe-52mm-pad...   \n",
       "3  https://www.saksfifthavenue.com/converse-babys...   \n",
       "4  https://www.saksfifthavenue.com/alexander-mcqu...   \n",
       "\n",
       "                                             details size  \n",
       "0  A modern pump, in a rounded silhouette with an...  NaN  \n",
       "1  Dress it down with jeans and sneakers or dress...  NaN  \n",
       "2  100% UV protection Case and cleaning cloth inc...  NaN  \n",
       "3  Canvas upper Round toe Lace-up vamp SmartFOAM ...  NaN  \n",
       "4  100% UV protection Gradient lenses Adjustable ...  NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we build our model. We need to preprocess the text to make it easier for the model to learn and predict outcome. In this step, we made product information (excluding product_id) lowercase, created a column that combines all product information including the brand name, product name, descriptions, etc., and removed stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make everything lowercase so that we won't have to worry about capitalization in analysis\n",
    "\n",
    "for i in my_df.columns[1:-1]:\n",
    "    my_df[i] = my_df[i].str.lower()\n",
    "    \n",
    "## Replace the slashes with space in brand_category to find key words easily\n",
    "my_df[\"brand_category\"] = my_df[\"brand_category\"].str.replace(\"/\",\" \")\n",
    "my_df[\"brand_category\"].fillna(value = 'unknown', inplace = True)\n",
    "\n",
    "\n",
    "### should predict based on brand, name, description, and details. Therefore, create a combined column\n",
    "product = []\n",
    "for i in range(len(my_df)):\n",
    "    product.append(str(my_df[\"brand\"][i]) + ' ' + str(my_df[\"name\"][i]) + ' ' \n",
    "                   + str(my_df[\"description\"][i]) + ' ' + str(my_df[\"details\"][i]))\n",
    "    \n",
    "my_df[\"product\"] = product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove stopwords\n",
    "\n",
    "stop = list(set(stopwords.words('english')))\n",
    "stop.extend([\"nan\",\",\",\"\",\"'\",\".\",\"@\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_token = []\n",
    "for i in my_df[\"product\"]:\n",
    "    product_token.append(word_tokenize(i))\n",
    "\n",
    "my_df[\"product_token\"] = product_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_removed_product = []\n",
    "\n",
    "for product in my_df[\"product_token\"]:\n",
    "    words = []\n",
    "    for word in product:\n",
    "        if word not in stop:\n",
    "            words.append(word)\n",
    "    stop_removed_product.append(words)\n",
    "\n",
    "info = []\n",
    "for i in stop_removed_product:\n",
    "    temp = \" \".join(i)\n",
    "    info.append(temp)\n",
    "    \n",
    "my_df[\"stop_removed_product\"] = info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find maternity and petite size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sizing category contains two very specific size labels, which are \"maternity\" and \"petite\". In the tagged label, there are very few products with the \"maternity\" label, and no product with the \"petite\" label. Note that in order for the model to train and pick up on characteristics related to petite products, there must be existing data with this label. Therefore, we conduct a manual search on items that explicitly have these tags in the product information. Although this is not the most appropriate approach, this step is added for the below two reasons:\n",
    "1. These tags are usually explicitly stated in the product name and directly refers to the product attribute (i.e. products with name stating \"maternity\" is usually explictly for pregnant women. Normal clothing does not typically have the description of \"maternity\" in its name.)\n",
    "2. The model needs to have all tags in its training data in order to generate predictions that includes each of the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizing = [\"maternity\", \"petite\"]\n",
    "\n",
    "for i in range(len(my_df)):\n",
    "    label = word_tokenize(my_df[\"name\"][i])\n",
    "    \n",
    "    for word in label:\n",
    "        if word in sizing:\n",
    "            my_df[\"size\"][i] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label non-clothing items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sizing tag only pertains to clothing items. Therefore, we must have a check that picks up non-clothing items such as jewelry, shoes and accessories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cloth = [\"sunglasse?\", \"shoe\",\"sneaker\",\"pump\",\"boot\",\"earring\",\"necklace\",\"bracelet\",\"belt\",\"bag\",\"wallet\",\n",
    "            \"backpack\",\"flat\",\"sandal\", \"makeup\", \"case\",\"scarf\", \"loafer\",\"\\shat\"]\n",
    "\n",
    "non_cloth_exp = \"|\".join(str(non_cloth[i]) + \"s?\" for i in range(len(non_cloth)))\n",
    "\n",
    "non_cloth_token = (r'\\b' + non_cloth_exp + r'\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the word embeddings for our data set. This means assigning each word in the product information a vector. The word embedding process references the functions and procedures outlined in week 7's \"Using RNNs and LSTMs.ipynb\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate out the training data, which is the products that have their sizing tags labeled by industry experts, as well as the maternity and petite products that we manually extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = my_df[my_df[\"size\"].isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "regular      2931\n",
       "petite        638\n",
       "maternity       8\n",
       "plus            2\n",
       "tall            1\n",
       "Name: size, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"size\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We assign the labels and document\n",
    "\n",
    "labels = train_df[\"size\"]\n",
    "docs = train_df[\"stop_removed_product\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the keras and sklearn library packages to encode the labels. Since the number of labels we have is limited (i.e. 5), we use the LabelEncoder package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain word embeddings, we must first tokenize the product information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, oov_token = \"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(train_df[\"stop_removed_product\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function that assigns integer as vectors for each word.\n",
    "\n",
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function that returns the maximum length of a product information in terms of number of tokens.\n",
    "\n",
    "def get_max_token_length_per_doc(docs):\n",
    "    return max(list(map(lambda x: len(x.split()), docs)))\n",
    "\n",
    "max_length = get_max_token_length_per_doc(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the number of time steps the model should back propagate. \n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "\n",
    "## We encode the training data, where the documents are each product's information (name, brand, description \n",
    "## and details)\n",
    "\n",
    "encoded_docs = integer_encode_documents(train_df[\"stop_removed_product\"].values, tokenizer)\n",
    "\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = padded_docs\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set a maximum vocabulary size as 1.1 times the number of unique words in the training data.\n",
    "\n",
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "## Load in the GloVe vectors.\n",
    "\n",
    "def load_glove_vectors():\n",
    "    embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "embeddings_index = load_glove_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a weight matrix for words in training docs.\n",
    "\n",
    "embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the training data set represented with word embeddings, we can train an LSTM model that takes the embeddings as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_classification_model(plot=False):\n",
    "    from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "    from keras.layers import Flatten, Masking\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0))\n",
    "    model.add(LSTM(units=32, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 100)          775100    \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 300, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 792,737\n",
      "Trainable params: 17,637\n",
      "Non-trainable params: 775,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "3580/3580 [==============================] - 26s 7ms/step - loss: 0.4482 - accuracy: 0.8447\n",
      "Epoch 2/5\n",
      "3580/3580 [==============================] - 24s 7ms/step - loss: 0.1445 - accuracy: 0.9528\n",
      "Epoch 3/5\n",
      "3580/3580 [==============================] - 24s 7ms/step - loss: 0.1017 - accuracy: 0.9701\n",
      "Epoch 4/5\n",
      "3580/3580 [==============================] - 25s 7ms/step - loss: 0.0755 - accuracy: 0.9793\n",
      "Epoch 5/5\n",
      "3580/3580 [==============================] - 25s 7ms/step - loss: 0.0694 - accuracy: 0.9827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1596cd210>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = make_lstm_classification_model()\n",
    "model.fit(X_train, y_train, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model trained, we define a function that takes a product information as strings and utilize the trained model to predict the sizing of the product. This function first preprocesses the input string by converting the words to lowercase and removing stopwords. It then checks if any part of the product information signifies that it is a non-clothing item. If not, it then uses the model to predict the sizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_size(doc):\n",
    "\n",
    "    stop = list(set(stopwords.words('english')))\n",
    "    stop.extend([\"nan\",\",\",\"\",\"'\",\".\",\"@\",\"/\",\":\",\";\",\"_\"])\n",
    "    \n",
    "    cleaned_txts = []\n",
    "    for text in doc:\n",
    "        words = word_tokenize(text)\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word in stop:\n",
    "                continue\n",
    "            new_words.append(word.lower())\n",
    "        cleaned_txt = \" \".join(new_words)\n",
    "        cleaned_txts.append(cleaned_txt)\n",
    "    \n",
    "    prod = [\" \".join(cleaned_txts[i] for i in range(len(cleaned_txts)))]\n",
    "    \n",
    "    if bool(re.findall(non_cloth_token, prod[0])) == True:\n",
    "        item = \"Not clothing item\"\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        encoded_text = integer_encode_documents(prod, tokenizer)\n",
    "        padded_text = pad_sequences(encoded_text, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "        prediction = model.predict_classes(padded_text)\n",
    "        item = encoder.inverse_transform(prediction)\n",
    "    \n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a product that's not a clothing item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not clothing item'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_size([\n",
    "    \"description: metal sunglasses in translucent tortoise pattern   140mm lens width; 54mm bridge width; 22mm temple length  100% uv protection  tinted lenses  adjustable nose pads  metal  made in italy\",\n",
    "    \"brand: prada\",\n",
    "    \"brand_category: translucent tortoise sunglasses\"\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a product that is a clothing item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['regular'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_size([\n",
    "    \"description: Our easy-fit shirt that skims your shape for a relaxed silhouette. Now in an updated, slimmer fit and a shorter length that‚Äôs perfect with high-waisted bottoms. Made with a soft satin fabric that uses a jacquard weaving technique to create a tonal leopard print. Spread collar. Long sleeves with buttoned cuffs. Button front placket. Back yoke seam with box pleats. Shirttail hem.\",\n",
    "    \"brand: Banana Republic\",\n",
    "    \"brand_category: Dillon classic-fit utility shirt\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
