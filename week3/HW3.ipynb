{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3\n",
    "\n",
    "Submit via Slack. Due on Monday, April 13th, 2020, 11:59pm PST. You may work with one other person.\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "You are an analyst working at Amazon as a product analyst, and charged with identifying areas for improvement to the Amazon toy product lines, which have been suffering recently from lower reviews.\n",
    "\n",
    "Using the **`poor_amazon_toy_reviews.txt`** and **`good_amazon_toy_reviews.txt`** datasets, clean and parse the text reviews. Explain the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "- what `n` for your `n-grams`?\n",
    "- which words to collocate together?\n",
    "\n",
    "Finally, generate a TF-IDF report that **visualizes**:\n",
    "* the features your analysis showed that customers cited as reasons for a 5 star review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief summary of the decision I made:\n",
    "- I removed stopwords because I think that those stopwords are not quite helpful for me to understand the reviews. In addition, due to the large dataset and high dimension, it is better to remove stopwords to save computational power.\n",
    "- I used lemmatization because I think it's important to take the original meaning and context into consideration when analyzing the reviews. \n",
    "- For the regex cleaning and substituion part, I replace the new line character with nothing because I don't want to lose information and don't want \\n to appear in the analysis since it is not helpful.\n",
    "- For custom stopwards, I added in several adverbs like probably, actually, really, etc. because I believe that the don't have any actual meaning.\n",
    "- For n-grams, I used n=2.\n",
    "\n",
    "For TF-IDF, it is simple to use and could return words that are descriptive and relevant to specific document. However, it is computationally intensive, especially when the vocabulary size is large. In addition, it does not take sequence of words into consideration, therfore, cannot capture the meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_file = open(\"/Users/yanboyang/Desktop/USC/20Spring/dso-560-nlp-and-text-analytics/week1/good_amazon_toy_reviews.txt\", \"r\")\n",
    "good_file.readline()\n",
    "poor_file = open(\"/Users/yanboyang/Desktop/USC/20Spring/dso-560-nlp-and-text-analytics/week1/poor_amazon_toy_reviews.txt\", \"r\")\n",
    "poor_file.readline()\n",
    "good = list(map(lambda review: review.replace('\\n', '').replace('\\\\\\\\', '').replace('/',''), good_file))\n",
    "poor = list(map(lambda review: review.replace('\\n', '').replace('\\\\\\\\', '').replace('/',''), poor_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english') + ['actually','usually','oh','always','thing','really','probably']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2,2),\n",
    "                             token_pattern=r'\\b[a-zA-Z0-9]{2,}\\b',\n",
    "                             min_df=5, stop_words=sw)\n",
    "X_good = vectorizer.fit_transform(good)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X_good.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "good_score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "#score[\"word\"] = terms\n",
    "good_score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>year old</th>\n",
       "      <td>1270.402445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daughter loves</th>\n",
       "      <td>857.957543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>son loves</th>\n",
       "      <td>812.171447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great product</th>\n",
       "      <td>788.753040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kids love</th>\n",
       "      <td>761.555360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      score\n",
       "year old        1270.402445\n",
       "daughter loves   857.957543\n",
       "son loves        812.171447\n",
       "great product    788.753040\n",
       "kids love        761.555360"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_score.to_csv(\"good.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import string\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "result = []\n",
    "for t in good:\n",
    "    words = word_tokenize(t)\n",
    "    final = []\n",
    "    for w in words:\n",
    "        final.append(lemmatizer.lemmatize(w).strip(string.punctuation))\n",
    "    res = \" \".join(final)\n",
    "    result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(sw + [\".\",'.', \",\",\":\", \"''\", \"'s\", \"'\", \"``\", \"(\", \")\", \"-\",\" \",\"\"])\n",
    "new_documents = []\n",
    "for review in result:\n",
    "    new_document = []\n",
    "    for word in review.split(' '):\n",
    "        if word.strip().lower() not in stopwords:\n",
    "            new_document.append(word)\n",
    "    new_documents.append(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('br', 'br'),\n",
       " ('year', 'old'),\n",
       " ('daughter', 'love'),\n",
       " ('son', 'love'),\n",
       " ('well', 'made'),\n",
       " ('ca', \"n't\"),\n",
       " ('doe', \"n't\"),\n",
       " ('kid', 'love'),\n",
       " ('old', 'love'),\n",
       " ('put', 'together'),\n",
       " ('3', 'year'),\n",
       " ('Great', 'product'),\n",
       " ('grandson', 'love'),\n",
       " ('good', 'quality'),\n",
       " ('2', 'year'),\n",
       " ('wa', 'great'),\n",
       " ('month', 'old'),\n",
       " ('much', 'fun'),\n",
       " ('lot', 'fun'),\n",
       " ('yr', 'old'),\n",
       " ('highly', 'recommend'),\n",
       " ('4', 'year'),\n",
       " ('absolutely', 'love'),\n",
       " ('birthday', 'party'),\n",
       " ('look', 'like')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "collocation_finder = BigramCollocationFinder.from_documents(new_documents)\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "collocation_finder.nbest(measures.raw_freq, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](goodviz.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poor Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2,2),\n",
    "                             token_pattern=r'\\b[a-zA-Z0-9]{2,}\\b',\n",
    "                             min_df=5, stop_words=sw)\n",
    "X_poor = vectorizer.fit_transform(poor)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X_poor.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "#score[\"word\"] = terms\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>waste money</th>\n",
       "      <td>256.075722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year old</th>\n",
       "      <td>131.394717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>br br</th>\n",
       "      <td>130.154481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor quality</th>\n",
       "      <td>124.699732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cheaply made</th>\n",
       "      <td>104.783140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fell apart</th>\n",
       "      <td>65.176357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would recommend</th>\n",
       "      <td>64.475933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first time</th>\n",
       "      <td>63.293833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looks like</th>\n",
       "      <td>61.654188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>put together</th>\n",
       "      <td>59.676626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>broke first</th>\n",
       "      <td>59.217094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth money</th>\n",
       "      <td>59.209674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopped working</th>\n",
       "      <td>56.687746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first day</th>\n",
       "      <td>56.377814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>piece junk</th>\n",
       "      <td>53.200208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poorly made</th>\n",
       "      <td>52.953129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>broke within</th>\n",
       "      <td>46.283510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like picture</th>\n",
       "      <td>44.984172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>send back</th>\n",
       "      <td>44.543070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look like</th>\n",
       "      <td>44.501047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      score\n",
       "waste money      256.075722\n",
       "year old         131.394717\n",
       "br br            130.154481\n",
       "poor quality     124.699732\n",
       "cheaply made     104.783140\n",
       "fell apart        65.176357\n",
       "would recommend   64.475933\n",
       "first time        63.293833\n",
       "looks like        61.654188\n",
       "put together      59.676626\n",
       "broke first       59.217094\n",
       "worth money       59.209674\n",
       "stopped working   56.687746\n",
       "first day         56.377814\n",
       "piece junk        53.200208\n",
       "poorly made       52.953129\n",
       "broke within      46.283510\n",
       "like picture      44.984172\n",
       "send back         44.543070\n",
       "look like         44.501047"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.to_csv(\"poor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "result = []\n",
    "for t in poor:\n",
    "    words = word_tokenize(t)\n",
    "    final = []\n",
    "    for w in words:\n",
    "        final.append(lemmatizer.lemmatize(w).strip(string.punctuation))\n",
    "    res = \" \".join(final)\n",
    "    result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = []\n",
    "for review in result:\n",
    "    new_document = []\n",
    "    for word in review.split(' '):\n",
    "        if word.strip().lower() not in stopwords:\n",
    "            new_document.append(word)\n",
    "    new_documents.append(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('br', 'br'),\n",
       " ('doe', \"n't\"),\n",
       " ('waste', 'money'),\n",
       " ('year', 'old'),\n",
       " (\"n't\", 'work'),\n",
       " (\"n't\", 'even'),\n",
       " ('ca', \"n't\"),\n",
       " ('would', \"n't\"),\n",
       " ('look', 'like'),\n",
       " ('wa', 'disappointed'),\n",
       " ('wa', \"n't\"),\n",
       " ('wo', \"n't\"),\n",
       " (\"n't\", 'buy'),\n",
       " (\"n't\", 'waste'),\n",
       " ('poor', 'quality'),\n",
       " ('could', \"n't\"),\n",
       " ('cheaply', 'made'),\n",
       " ('put', 'together'),\n",
       " ('first', 'time'),\n",
       " (\"n't\", 'get'),\n",
       " ('wa', 'excited'),\n",
       " (\"n't\", 'stay'),\n",
       " ('thought', 'wa'),\n",
       " ('first', 'day'),\n",
       " ('stopped', 'working')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "collocation_finder = BigramCollocationFinder.from_documents(new_documents)\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "collocation_finder.nbest(measures.raw_freq, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](poorviz.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Attribution (Feature Engineering and Regex Practice)\n",
    "\n",
    "Download the [dataset](https://dso-560-nlp-text-analytics.s3.amazonaws.com/truncated_catalog.csv) from the class S3 bucket (`dso560-nlp-text-analytics`).\n",
    "\n",
    "In preparation for the group project, our client company has provided a dataset of women's clothing products they are considering cataloging. \n",
    "\n",
    "1. Filter for only **women's clothing items**.\n",
    "\n",
    "2. For each clothing item:\n",
    "\n",
    "* Identify its **category**:\n",
    "```\n",
    "Bottom\n",
    "One Piece\n",
    "Shoe\n",
    "Handbag\n",
    "Scarf\n",
    "```\n",
    "* Identify its **color**:\n",
    "```\n",
    "Beige\n",
    "Black\n",
    "Blue\n",
    "Brown\n",
    "Burgundy\n",
    "Gold\n",
    "Gray\n",
    "Green\n",
    "Multi \n",
    "Navy\n",
    "Neutral\n",
    "Orange\n",
    "Pinks\n",
    "Purple\n",
    "Red\n",
    "Silver\n",
    "Teal\n",
    "White\n",
    "Yellow\n",
    "```\n",
    "\n",
    "Your output will be the same dataset, except with **3 additional fields**:\n",
    "* `is_womens_clothing`\n",
    "* `product_category`\n",
    "* `colors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"truncated_catalog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to stackoverflow\n",
    "dataset['all_info'] = dataset[dataset.columns[1:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str).str.lower()),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dataset['woman']=dataset['all_info'].str.findall(r'\\b(woman|women|girl|grils|female|wife|daughter|daughters|girlfriend|girlfriends|mother|mom|mommy|mommies)\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def func(x):\n",
    "    if len(x) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "dataset['is_womens_clothing']=dataset['woman'].apply(lambda x: func(x))\n",
    "dataset.drop('woman',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['cat']=dataset['all_info'].str.findall(r'(bottom|shoe|handbag|scarf|one piece|one-piece|onepiece|hand bag)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['category'] = dataset['cat'].apply(lambda x: list(set(x)))\n",
    "dataset.drop('cat', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>brand_canonical_url</th>\n",
       "      <th>details</th>\n",
       "      <th>tsv</th>\n",
       "      <th>all_info</th>\n",
       "      <th>is_womens_clothing</th>\n",
       "      <th>category</th>\n",
       "      <th>colors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FILA</td>\n",
       "      <td>Original Fitness Sneakers</td>\n",
       "      <td>Vintage Fitness leather sneakers with logo pri...</td>\n",
       "      <td>TheMensStore/Shoes/Sneakers/LowTop</td>\n",
       "      <td>https://www.saksfifthavenue.com/fila-original-...</td>\n",
       "      <td>Leather/synthetic upper\\nLace-up closure\\nText...</td>\n",
       "      <td>'design':12 'fila':1A 'fit':3A,6 'leather':7 '...</td>\n",
       "      <td>original fitness sneakers,vintage fitness leat...</td>\n",
       "      <td>False</td>\n",
       "      <td>[shoe]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHANEL</td>\n",
       "      <td>HAT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>https://www.saksfifthavenue.com/chanel-hat/pro...</td>\n",
       "      <td>WOOL TWEED &amp; FELT</td>\n",
       "      <td>'chanel':1A 'hat':2A</td>\n",
       "      <td>hat,unknown,https://www.saksfifthavenue.com/ch...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frame</td>\n",
       "      <td>Petit Oval Buckle Belt</td>\n",
       "      <td>A Timeless Leather Belt Crafted From Smooth Co...</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>https://frame-store.com/products/petit-oval-bu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>'belt':5A,9 'buckl':4A,21 'cowhid':13 'craft':...</td>\n",
       "      <td>petit oval buckle belt,a timeless leather belt...</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[multi, gold]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lilly Pulitzer Kids</td>\n",
       "      <td>Little Gir's &amp; Girl's Ariana One-Piece UPF 50+...</td>\n",
       "      <td>Pretty ruffle sleeves and trim elevate essenti...</td>\n",
       "      <td>JustKids/Girls214/Girls/SwimwearCoverups,JustK...</td>\n",
       "      <td>https://www.saksfifthavenue.com/lilly-pulitzer...</td>\n",
       "      <td>Scoopneck\\nAdjustable straps\\nFlutter sleeves\\...</td>\n",
       "      <td>'50':14A 'allov':28 'ariana':9A 'color':27 'el...</td>\n",
       "      <td>little gir's &amp; girl's ariana one-piece upf 50+...</td>\n",
       "      <td>True</td>\n",
       "      <td>[one-piece]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kissy Kissy</td>\n",
       "      <td>Baby Girl's Endearing Elephants Pima Cotton Co...</td>\n",
       "      <td>Versatile convertible gown with elephant applique</td>\n",
       "      <td>JustKids/Baby024months/InfantGirls/FootiesRompers</td>\n",
       "      <td>https://www.saksfifthavenue.com/kissy-kissy-ba...</td>\n",
       "      <td>V-neckline\\nLong sleeves\\nFront snap closure\\n...</td>\n",
       "      <td>'appliqu':17 'babi':3A 'convert':10A,13 'cotto...</td>\n",
       "      <td>baby girl's endearing elephants pima cotton co...</td>\n",
       "      <td>True</td>\n",
       "      <td>[bottom]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 brand                                               name  \\\n",
       "0                 FILA                          Original Fitness Sneakers   \n",
       "1               CHANEL                                                HAT   \n",
       "2                Frame                             Petit Oval Buckle Belt   \n",
       "3  Lilly Pulitzer Kids  Little Gir's & Girl's Ariana One-Piece UPF 50+...   \n",
       "4          Kissy Kissy  Baby Girl's Endearing Elephants Pima Cotton Co...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Vintage Fitness leather sneakers with logo pri...   \n",
       "1                                                NaN   \n",
       "2  A Timeless Leather Belt Crafted From Smooth Co...   \n",
       "3  Pretty ruffle sleeves and trim elevate essenti...   \n",
       "4  Versatile convertible gown with elephant applique   \n",
       "\n",
       "                                      brand_category  \\\n",
       "0                 TheMensStore/Shoes/Sneakers/LowTop   \n",
       "1                                            Unknown   \n",
       "2                                        Accessories   \n",
       "3  JustKids/Girls214/Girls/SwimwearCoverups,JustK...   \n",
       "4  JustKids/Baby024months/InfantGirls/FootiesRompers   \n",
       "\n",
       "                                 brand_canonical_url  \\\n",
       "0  https://www.saksfifthavenue.com/fila-original-...   \n",
       "1  https://www.saksfifthavenue.com/chanel-hat/pro...   \n",
       "2  https://frame-store.com/products/petit-oval-bu...   \n",
       "3  https://www.saksfifthavenue.com/lilly-pulitzer...   \n",
       "4  https://www.saksfifthavenue.com/kissy-kissy-ba...   \n",
       "\n",
       "                                             details  \\\n",
       "0  Leather/synthetic upper\\nLace-up closure\\nText...   \n",
       "1                                  WOOL TWEED & FELT   \n",
       "2                                                NaN   \n",
       "3  Scoopneck\\nAdjustable straps\\nFlutter sleeves\\...   \n",
       "4  V-neckline\\nLong sleeves\\nFront snap closure\\n...   \n",
       "\n",
       "                                                 tsv  \\\n",
       "0  'design':12 'fila':1A 'fit':3A,6 'leather':7 '...   \n",
       "1                               'chanel':1A 'hat':2A   \n",
       "2  'belt':5A,9 'buckl':4A,21 'cowhid':13 'craft':...   \n",
       "3  '50':14A 'allov':28 'ariana':9A 'color':27 'el...   \n",
       "4  'appliqu':17 'babi':3A 'convert':10A,13 'cotto...   \n",
       "\n",
       "                                            all_info  is_womens_clothing  \\\n",
       "0  original fitness sneakers,vintage fitness leat...               False   \n",
       "1  hat,unknown,https://www.saksfifthavenue.com/ch...               False   \n",
       "2  petit oval buckle belt,a timeless leather belt...               False   \n",
       "3  little gir's & girl's ariana one-piece upf 50+...                True   \n",
       "4  baby girl's endearing elephants pima cotton co...                True   \n",
       "\n",
       "      category         colors  \n",
       "0       [shoe]             []  \n",
       "1           []             []  \n",
       "2           []  [multi, gold]  \n",
       "3  [one-piece]             []  \n",
       "4     [bottom]             []  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['color']=dataset['all_info'].str.findall(r'(beige|black|blue|brown|burgundy|gold|gray|green|multi|navy|neutral|orange|pink|pinks|purple|red|silver|teal|white|yellow)')\n",
    "dataset['colors'] = dataset['color'].apply(lambda x: list(set(x)))\n",
    "dataset.drop('color', axis=1, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
