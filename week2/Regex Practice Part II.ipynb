{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Dependencies-and-Load-in-Dataset\" data-toc-modified-id=\"Import-Dependencies-and-Load-in-Dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Dependencies and Load in Dataset</a></span></li><li><span><a href=\"#Pandas-Exploratory-Data-Analysis\" data-toc-modified-id=\"Pandas-Exploratory-Data-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pandas Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Show-the-First/Last-Rows-of-Dataset\" data-toc-modified-id=\"Show-the-First/Last-Rows-of-Dataset-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Show the First/Last Rows of Dataset</a></span></li><li><span><a href=\"#Drop-Unnecessary-Pandas-Column\" data-toc-modified-id=\"Drop-Unnecessary-Pandas-Column-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Drop Unnecessary Pandas Column</a></span></li><li><span><a href=\"#Compute-a-New-Pandas-Series-and-Attach-to-Dataframe-as-Column\" data-toc-modified-id=\"Compute-a-New-Pandas-Series-and-Attach-to-Dataframe-as-Column-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Compute a New Pandas Series and Attach to Dataframe as Column</a></span></li><li><span><a href=\"#Count-Number-of-Rows-in-Dataframe-Meet-Some-Criteria\" data-toc-modified-id=\"Count-Number-of-Rows-in-Dataframe-Meet-Some-Criteria-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>Count Number of Rows in Dataframe Meet Some Criteria</a></span></li><li><span><a href=\"#Filter-DataFrame-for-a-Subset-of-Rows\" data-toc-modified-id=\"Filter-DataFrame-for-a-Subset-of-Rows-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>Filter DataFrame for a Subset of Rows</a></span></li></ul></li><li><span><a href=\"#Difference-Between-extractall-and-findall\" data-toc-modified-id=\"Difference-Between-extractall-and-findall-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Difference Between <code>extractall</code> and <code>findall</code></a></span></li></ul></li><li><span><a href=\"#Regex-Character-Classes\" data-toc-modified-id=\"Regex-Character-Classes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Regex Character Classes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Find-all-Tweets-That-Start-with-a-Number\" data-toc-modified-id=\"Find-all-Tweets-That-Start-with-a-Number-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Find all Tweets That Start with a Number</a></span></li><li><span><a href=\"#Find-all-@-Mentions\" data-toc-modified-id=\"Find-all-@-Mentions-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Find all @ Mentions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Alternative-Method-to-Work-With-Lists-in-Pandas\" data-toc-modified-id=\"Alternative-Method-to-Work-With-Lists-in-Pandas-3.0.2.1\"><span class=\"toc-item-num\">3.0.2.1&nbsp;&nbsp;</span>Alternative Method to Work With Lists in Pandas</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Quantifiers\" data-toc-modified-id=\"Quantifiers-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Quantifiers</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Match-All-Phone-Numbers-(Link)\" data-toc-modified-id=\"Match-All-Phone-Numbers-(Link)-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Match All Phone Numbers (<a href=\"https://regexr.com/50v17\" target=\"_blank\">Link</a>)</a></span></li><li><span><a href=\"#Parse-Out-Zip-Codes-(Link)\" data-toc-modified-id=\"Parse-Out-Zip-Codes-(Link)-4.0.2\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>Parse Out Zip Codes (<a href=\"https://regexr.com/50v1g\" target=\"_blank\">Link</a>)</a></span></li></ul></li></ul></li><li><span><a href=\"#Capture-Groups\" data-toc-modified-id=\"Capture-Groups-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Capture Groups</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example:-Parsing-out-Weekday-from-Timestamp-String-(Link)\" data-toc-modified-id=\"Example:-Parsing-out-Weekday-from-Timestamp-String-(Link)-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>Example: Parsing out Weekday from Timestamp String (<a href=\"https://regexr.com/50v0l\" target=\"_blank\">Link</a>)</a></span></li><li><span><a href=\"#Example:-Parsing-Out-Domain-Names\" data-toc-modified-id=\"Example:-Parsing-Out-Domain-Names-5.0.2\"><span class=\"toc-item-num\">5.0.2&nbsp;&nbsp;</span>Example: Parsing Out Domain Names</a></span></li></ul></li><li><span><a href=\"#Non-Capture-Groups\" data-toc-modified-id=\"Non-Capture-Groups-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Non-Capture Groups</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Link)\" data-toc-modified-id=\"Example-1-(Link)-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Example 1 <a href=\"https://regexr.com/50t7c\" target=\"_blank\">(Link)</a></a></span></li><li><span><a href=\"#Example-2-(Link)\" data-toc-modified-id=\"Example-2-(Link)-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Example 2 <a href=\"https://regexr.com/50t7c\" target=\"_blank\">(Link)</a></a></span></li><li><span><a href=\"#Example-3-(Link)\" data-toc-modified-id=\"Example-3-(Link)-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Example 3 (<a href=\"https://regexr.com/50ush\" target=\"_blank\">Link</a>)</a></span></li></ul></li></ul></li><li><span><a href=\"#Exercises\" data-toc-modified-id=\"Exercises-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Exercises</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Find-the-subject-headings-for-these-emails.\" data-toc-modified-id=\"Find-the-subject-headings-for-these-emails.-6.0.0.1\"><span class=\"toc-item-num\">6.0.0.1&nbsp;&nbsp;</span>Find the subject headings for these emails.</a></span></li><li><span><a href=\"#Find-all-hashtags-mentioned-in-the-tweets_df-dataset.-Store-it-as-a-separate-column-called-hashtags.\" data-toc-modified-id=\"Find-all-hashtags-mentioned-in-the-tweets_df-dataset.-Store-it-as-a-separate-column-called-hashtags.-6.0.0.2\"><span class=\"toc-item-num\">6.0.0.2&nbsp;&nbsp;</span>Find all hashtags mentioned in the <code>tweets_df</code> dataset. Store it as a separate column called <strong>hashtags</strong>.</a></span></li><li><span><a href=\"#In-fraudulent_emails.txt,-identify-the-list-of-email-addresses-for-your-security-administrator-to-blacklist-from-your-company's-email-servers.\" data-toc-modified-id=\"In-fraudulent_emails.txt,-identify-the-list-of-email-addresses-for-your-security-administrator-to-blacklist-from-your-company's-email-servers.-6.0.0.3\"><span class=\"toc-item-num\">6.0.0.3&nbsp;&nbsp;</span>In <code>fraudulent_emails.txt</code>, identify the <strong>list of email addresses</strong> for your security administrator to blacklist from your company's email servers.</a></span></li><li><span><a href=\"#In-fraudulent_emails.txt,-identify-any-IP-addresses-that-should-be-blacklisted\" data-toc-modified-id=\"In-fraudulent_emails.txt,-identify-any-IP-addresses-that-should-be-blacklisted-6.0.0.4\"><span class=\"toc-item-num\">6.0.0.4&nbsp;&nbsp;</span>In <code>fraudulent_emails.txt</code>, identify any IP addresses that should be blacklisted</a></span></li><li><span><a href=\"#The-word-&quot;AT&amp;T&quot;-is-not-spelled-correctly-in-the-tweets_df-dataset.-Correct-the-misspelling.\" data-toc-modified-id=\"The-word-&quot;AT&amp;T&quot;-is-not-spelled-correctly-in-the-tweets_df-dataset.-Correct-the-misspelling.-6.0.0.5\"><span class=\"toc-item-num\">6.0.0.5&nbsp;&nbsp;</span>The word \"AT&amp;T\" is not spelled correctly in the <code>tweets_df</code> dataset. Correct the misspelling.</a></span></li><li><span><a href=\"#Removing-hashtags-and-mentions,-which-topic-has-the-highest-average-tweet-character-count?\" data-toc-modified-id=\"Removing-hashtags-and-mentions,-which-topic-has-the-highest-average-tweet-character-count?-6.0.0.6\"><span class=\"toc-item-num\">6.0.0.6&nbsp;&nbsp;</span>Removing hashtags and mentions, which topic has the highest average tweet character count?</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies and Load in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets_df: pd.DataFrame = pd.read_csv(\"tweets_pandas.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the First/Last Rows of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   likes  index                          date    topic    handle  \\\n",
       "0      4      3  Mon May 11 03:17:40 UTC 2009  kindle2    tpryan   \n",
       "1      4      4  Mon May 11 03:18:03 UTC 2009  kindle2    vcu451   \n",
       "2      4      5  Mon May 11 03:18:54 UTC 2009  kindle2    chadfu   \n",
       "3      4      6  Mon May 11 03:19:04 UTC 2009  kindle2     SIX15   \n",
       "4      4      7  Mon May 11 03:21:41 UTC 2009  kindle2  yamarama   \n",
       "\n",
       "                                               tweet  \n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2</td>\n",
       "      <td>14072</td>\n",
       "      <td>Sun Jun 14 04:31:43 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>proggit</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>14073</td>\n",
       "      <td>Sun Jun 14 04:32:17 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>sam33r</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>4</td>\n",
       "      <td>14074</td>\n",
       "      <td>Sun Jun 14 04:36:34 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>iamtheonlyjosie</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>14075</td>\n",
       "      <td>Sun Jun 14 21:36:07 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>plutopup7</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>14076</td>\n",
       "      <td>Sun Jun 14 21:36:17 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>captain_pete</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     likes  index                          date  topic           handle  \\\n",
       "493      2  14072  Sun Jun 14 04:31:43 UTC 2009  latex          proggit   \n",
       "494      0  14073  Sun Jun 14 04:32:17 UTC 2009  latex           sam33r   \n",
       "495      4  14074  Sun Jun 14 04:36:34 UTC 2009  latex  iamtheonlyjosie   \n",
       "496      0  14075  Sun Jun 14 21:36:07 UTC 2009   iran        plutopup7   \n",
       "497      0  14076  Sun Jun 14 21:36:17 UTC 2009   iran     captain_pete   \n",
       "\n",
       "                                                 tweet  \n",
       "493  Ask Programming: LaTeX or InDesign?: submitted...  \n",
       "494  On that note, I hate Word. I hate Pages. I hat...  \n",
       "495  Ahhh... back in a *real* text editing environm...  \n",
       "496  Trouble in Iran, I see. Hmm. Iran. Iran so far...  \n",
       "497  Reading the tweets coming out of Iran... The w...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Unnecessary Pandas Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a New Pandas Series and Attach to Dataframe as Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length of tweets in characters\n",
    "tweets_df['length'] = tweets_df['tweet'].apply(len)\n",
    "#tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Number of Rows in Dataframe Meet Some Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of times Obama appears in tweets\n",
    "d = tweets_df['tweet'].apply(lambda x: 'obama' in x.lower())\n",
    "sum(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter DataFrame for a Subset of Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama = tweets_df[d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between `extractall` and `findall`\n",
    "\n",
    "The `extractall` method will return a fanned-out multi-dimensional index. For example, in the example below, the primary index is `0`, but there are sub-indices for `stellargirl` (`0`), `loooooooovvvvvveee` (`1`), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>stellargirl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loooooooovvvvvveee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kindle2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "  match                    \n",
       "0 0             stellargirl\n",
       "  1      loooooooovvvvvveee\n",
       "  2                 Kindle2\n",
       "  3                     Not\n",
       "  4                    that"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet\"].str.extractall(r'\\b(\\w{3,})\\b').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `findall` method will return a list of results (or an empty list if there is no match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [stellargirl, loooooooovvvvvveee, Kindle2, Not...\n",
       "1    [Reading, kindle2, Love, Lee, childs, good, read]\n",
       "2     [first, assesment, the, kindle2, fucking, rocks]\n",
       "3    [kenburbary, You, love, your, Kindle2, had, mi...\n",
       "4    [mikefish, Fair, enough, But, have, the, Kindl...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet\"].str.findall(r'\\b(\\w{3,})\\b').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex Character Classes\n",
    "\n",
    "There are often shortcut keywords you can use instead of typing out every possible character you want to match against. For instance, instead of `[a-zA-Z0-9]`, for all practical purposes, you can type out `/w`.\n",
    "\n",
    "![Character Classes](images/character_chars.png \"Regex character classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all Tweets That Start with a Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Non-Match Example](https://regexr.com/50ur7)\n",
    "* [Match Example](https://regexr.com/50urj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242    45 Pros You Should Be Following on Twitter - h...\n",
       "430    10 tips for healthy eating ? ResultsBy Fitness...\n",
       "482    7 hours. 7 hours of inkscape crashing, normall...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "tweets_df['num'] = tweets_df['tweet'].str.findall(r'^[0-9]')\n",
    "tweets_slctd = tweets_df[tweets_df.num.map(len)>0]['tweet']\n",
    "tweets_slctd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all @ Mentions\n",
    "We'll use the `\\w` character class to match for mentions (ie. `@ychennay`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
       "3     @kenburbary You'll love your Kindle2. I've had...\n",
       "4     @mikefish  Fair enough. But i have the Kindle2...\n",
       "5     @richardebaker no. it is too big. I'm quite ha...\n",
       "11    @Karoli I firmly believe that Obama/Pelosi hav...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['mention'] = tweets_df['tweet'].str.findall(r'@\\w+')\n",
    "tweets_slctd = tweets_df[tweets_df.mention.map(len)>0]\n",
    "tweets_slctd['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Method to Work With Lists in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
       "3     @kenburbary You'll love your Kindle2. I've had...\n",
       "4     @mikefish  Fair enough. But i have the Kindle2...\n",
       "5     @richardebaker no. it is too big. I'm quite ha...\n",
       "11    @Karoli I firmly believe that Obama/Pelosi hav...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[tweets_df['tweet'].map(lambda t: '@' in t)]['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantifiers\n",
    "\n",
    "Quantifiers let you specify how many times a character or group should be matched.\n",
    "![Quantifiers](images/quantifiers.png \"Regex character classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Match All Phone Numbers ([Link](https://regexr.com/50v17))\n",
    "In the unwanted callers dataset (`unwanted_calls.csv`), parse out all phone numbers.\n",
    "\n",
    "We'll only consider for the time being phone numbers that follow the format `123-456-7890`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>ticket_created</th>\n",
       "      <th>date_of_issue</th>\n",
       "      <th>time_of_issue</th>\n",
       "      <th>form</th>\n",
       "      <th>method</th>\n",
       "      <th>issue</th>\n",
       "      <th>caller_id_number</th>\n",
       "      <th>type_of_call_or_messge</th>\n",
       "      <th>advertiser_business_number</th>\n",
       "      <th>state</th>\n",
       "      <th>zip</th>\n",
       "      <th>location_center_point_of_the_zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000296</td>\n",
       "      <td>05/25/2016 11:15:29 AM +0000</td>\n",
       "      <td>5/1/16</td>\n",
       "      <td>1:00 PM</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Internet (VOIP)</td>\n",
       "      <td>Robocalls</td>\n",
       "      <td>866-410-0458</td>\n",
       "      <td>Autodialed Live Voice Call</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FL</td>\n",
       "      <td>33324.0</td>\n",
       "      <td>FL 33324\\n(26.11294, -80.27429)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000319</td>\n",
       "      <td>05/25/2016 12:51:35 PM +0000</td>\n",
       "      <td>3/7/16</td>\n",
       "      <td>12:00 PM</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Wired</td>\n",
       "      <td>Telemarketing (including do not call and spoof...</td>\n",
       "      <td>619-840-7262</td>\n",
       "      <td>Live Voice</td>\n",
       "      <td>619-840-7262</td>\n",
       "      <td>CA</td>\n",
       "      <td>92078.0</td>\n",
       "      <td>CA 92078\\n(33.122635, -117.190612)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000322</td>\n",
       "      <td>05/25/2016 12:56:54 PM +0000</td>\n",
       "      <td>5/24/16</td>\n",
       "      <td>8:08 PM</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Wireless (cell phone/other mobile device)</td>\n",
       "      <td>Telemarketing (including do not call and spoof...</td>\n",
       "      <td>626-691-9090</td>\n",
       "      <td>Live Voice</td>\n",
       "      <td>626-691-9090</td>\n",
       "      <td>NJ</td>\n",
       "      <td>7481.0</td>\n",
       "      <td>NJ 07481\\n(40.998076, -74.167269)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000323</td>\n",
       "      <td>05/25/2016 01:00:22 PM +0000</td>\n",
       "      <td>3/7/16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Wired</td>\n",
       "      <td>Telemarketing (including do not call and spoof...</td>\n",
       "      <td>877-218-8361</td>\n",
       "      <td>Abandoned Calls</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MA</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>MA 01748\\n(42.224925, -71.537489)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000326</td>\n",
       "      <td>05/25/2016 01:02:31 PM +0000</td>\n",
       "      <td>5/25/16</td>\n",
       "      <td>7:24 PM</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Internet (VOIP)</td>\n",
       "      <td>Telemarketing (including do not call and spoof...</td>\n",
       "      <td>877-705-6767</td>\n",
       "      <td>Abandoned Calls</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IL</td>\n",
       "      <td>60504.0</td>\n",
       "      <td>IL 60504-8149\\n(41.771365, -88.226673)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id                ticket_created date_of_issue time_of_issue   form  \\\n",
       "0    1000296  05/25/2016 11:15:29 AM +0000        5/1/16       1:00 PM  Phone   \n",
       "1    1000319  05/25/2016 12:51:35 PM +0000        3/7/16      12:00 PM  Phone   \n",
       "2    1000322  05/25/2016 12:56:54 PM +0000       5/24/16       8:08 PM  Phone   \n",
       "3    1000323  05/25/2016 01:00:22 PM +0000        3/7/16           NaN  Phone   \n",
       "4    1000326  05/25/2016 01:02:31 PM +0000       5/25/16       7:24 PM  Phone   \n",
       "\n",
       "                                      method  \\\n",
       "0                            Internet (VOIP)   \n",
       "1                                      Wired   \n",
       "2  Wireless (cell phone/other mobile device)   \n",
       "3                                      Wired   \n",
       "4                            Internet (VOIP)   \n",
       "\n",
       "                                               issue caller_id_number  \\\n",
       "0                                          Robocalls     866-410-0458   \n",
       "1  Telemarketing (including do not call and spoof...     619-840-7262   \n",
       "2  Telemarketing (including do not call and spoof...     626-691-9090   \n",
       "3  Telemarketing (including do not call and spoof...     877-218-8361   \n",
       "4  Telemarketing (including do not call and spoof...     877-705-6767   \n",
       "\n",
       "       type_of_call_or_messge advertiser_business_number state      zip  \\\n",
       "0  Autodialed Live Voice Call                        NaN    FL  33324.0   \n",
       "1                  Live Voice               619-840-7262    CA  92078.0   \n",
       "2                  Live Voice               626-691-9090    NJ   7481.0   \n",
       "3             Abandoned Calls                        NaN    MA   1748.0   \n",
       "4             Abandoned Calls                        NaN    IL  60504.0   \n",
       "\n",
       "    location_center_point_of_the_zip_code  \n",
       "0         FL 33324\\n(26.11294, -80.27429)  \n",
       "1      CA 92078\\n(33.122635, -117.190612)  \n",
       "2       NJ 07481\\n(40.998076, -74.167269)  \n",
       "3       MA 01748\\n(42.224925, -71.537489)  \n",
       "4  IL 60504-8149\\n(41.771365, -88.226673)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calls = pd.read_csv(\"unwanted_calls.csv\")\n",
    "calls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [866-410-0458]\n",
       "1    [619-840-7262]\n",
       "2    [626-691-9090]\n",
       "3    [877-218-8361]\n",
       "4    [877-705-6767]\n",
       "Name: caller_id_number, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calls['caller_id_number'].str.findall(r'\\d{3}-\\d{3}-\\d{4}').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "### Parse Out Zip Codes ([Link](https://regexr.com/50v1g))\n",
    "In the `location_center_point_of_the_zip_code` field, we store both zip codes as well as geolocation data (latitudes and longitudes). We'll only consider zip codes with 5 digits (not the +4 digit delivery route)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [ 33324\\n]\n",
       "1    [ 92078\\n]\n",
       "2    [ 07481\\n]\n",
       "3    [ 01748\\n]\n",
       "4            []\n",
       "Name: location_center_point_of_the_zip_code, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calls['location_center_point_of_the_zip_code'].str.findall(r'\\s\\d{5}\\s').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture Groups\n",
    "\n",
    "[Oracle documentation on Capture Groups:](https://docs.oracle.com/javase/tutorial/essential/regex/groups.html)\n",
    "> Capturing groups are a way to treat **multiple characters as a single unit**. They are created by placing the characters to be grouped inside a set of parentheses. For example, the regular expression `(dog)` creates a single group containing the letters `d`, `o`, and `g`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Parsing out Weekday from Timestamp String ([Link](https://regexr.com/50v0l))\n",
    "\n",
    "In the `date` field of `tweets_df`, we have timestamp strings that look like this: `Mon May 11 03:22:30 UTC 2009`. We want to parse out the weekday from this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Mon]\n",
       "1    [Mon]\n",
       "2    [Mon]\n",
       "3    [Mon]\n",
       "4    [Mon]\n",
       "Name: date, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['date'].str.findall(r'^\\w{3}\\b').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Parsing Out Domain Names\n",
    "We want to capture the domain names of different websites. Here, we need to escape the `.` part of `www.google.com`. In regex, `.` means \"anything\". To actually indicate we want to match for the literal `.` period character, we need to escape it, using `\\.`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = open(\"list_of_websites.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [.youtube.]\n",
       "1    [.facebook.]\n",
       "2       [.baidu.]\n",
       "3       [.yahoo.]\n",
       "4      [.amazon.]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web = pd.DataFrame(websites)\n",
    "web[0].str.findall(r'\\.\\w+\\.').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Capture Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want to group multiple characters into a single unit to apply regex operations on them, but you don't\n",
    "want to actually capture or return their result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 [(Link)](https://regexr.com/50t7c)\n",
    "Using non-capture groups to match for optional text:\n",
    "\n",
    "You want to capture both `child` and `children` in your text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320    #jobs #sittercity Help with taking care of sic...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['child'] = tweets_df['tweet'].str.findall(r'\\bchild(?:ren)?\\b')\n",
    "tweets_slctd = tweets_df[tweets_df.child.map(len)>0]\n",
    "tweets_slctd['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 [(Link)](https://regexr.com/50t7c)\n",
    "Find all the mentions in the tweets. Mentions start with `@`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
       "3     @kenburbary You'll love your Kindle2. I've had...\n",
       "4     @mikefish  Fair enough. But i have the Kindle2...\n",
       "5     @richardebaker no. it is too big. I'm quite ha...\n",
       "11    @Karoli I firmly believe that Obama/Pelosi hav...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['mention'] = tweets_df['tweet'].str.findall(r'@\\w+')\n",
    "tweets_slctd = tweets_df[tweets_df.mention.map(len)>0]\n",
    "tweets_slctd['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 ([Link](https://regexr.com/50ush))\n",
    "Here we want to match for the dollar amount, but we don't want to include the currency notation (`$` or `USD`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Average fast food wage is $9.08, but inflation has increased USD9.23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9', '08'), ('9', '23')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'(?:\\$|(?:USD))([0-9]+)\\.([0-9]{2})', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "You'll be using the the `tweets_df` Pandas dataframe and `fraudulent_emails.txt` text files for these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_text = open(\"fraudulent_emails.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the subject headings for these emails.\n",
    "**Hint**: Look for the subject line within the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' PLEASE',\n",
       " ' ASSISTANCETo',\n",
       " ' PLEASE',\n",
       " ' RespondTo',\n",
       " ' NICE',\n",
       " ' PROJECT',\n",
       " ' GOOD',\n",
       " ' Fr',\n",
       " ' SEED',\n",
       " ' from',\n",
       " 'To',\n",
       " ' From']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = email_text.replace('\\n','')\n",
    "re.findall(r'(?<=Subject:)\\s*\\w+', e)\n",
    "\n",
    "#(?<=This is)(.*)(?=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all hashtags mentioned in the `tweets_df` dataset. Store it as a separate column called **hashtags**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>num</th>\n",
       "      <th>mention</th>\n",
       "      <th>child</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "      <td>58</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#kindle2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 05:20:15 UTC 2009</td>\n",
       "      <td>lebron</td>\n",
       "      <td>cameronwylie</td>\n",
       "      <td>#lebron best athlete of our generation, if not...</td>\n",
       "      <td>135</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#lebron]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 05:21:45 UTC 2009</td>\n",
       "      <td>lebron</td>\n",
       "      <td>Native_01</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>6</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#NAME]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 05:22:37 UTC 2009</td>\n",
       "      <td>lebron</td>\n",
       "      <td>emceet</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>6</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#NAME]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>Thu May 14 02:58:07 UTC 2009</td>\n",
       "      <td>\"booz allen\"</td>\n",
       "      <td>JoeSchueller</td>\n",
       "      <td>Booz Allen Hamilton has a bad ass homegrown so...</td>\n",
       "      <td>91</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#ttiv]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    likes                          date         topic        handle  \\\n",
       "2       4  Mon May 11 03:18:54 UTC 2009       kindle2        chadfu   \n",
       "15      4  Mon May 11 05:20:15 UTC 2009        lebron  cameronwylie   \n",
       "19      4  Mon May 11 05:21:45 UTC 2009        lebron     Native_01   \n",
       "22      4  Mon May 11 05:22:37 UTC 2009        lebron        emceet   \n",
       "27      4  Thu May 14 02:58:07 UTC 2009  \"booz allen\"  JoeSchueller   \n",
       "\n",
       "                                                tweet  length num mention  \\\n",
       "2   Ok, first assesment of the #kindle2 ...it fuck...      58  []      []   \n",
       "15  #lebron best athlete of our generation, if not...     135  []      []   \n",
       "19                                             #NAME?       6  []      []   \n",
       "22                                             #NAME?       6  []      []   \n",
       "27  Booz Allen Hamilton has a bad ass homegrown so...      91  []      []   \n",
       "\n",
       "   child    hashtags  \n",
       "2     []  [#kindle2]  \n",
       "15    []   [#lebron]  \n",
       "19    []     [#NAME]  \n",
       "22    []     [#NAME]  \n",
       "27    []     [#ttiv]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['hashtags'] = tweets_df['tweet'].str.findall(r'#\\w+')\n",
    "tweets_slctd = tweets_df[tweets_df.hashtags.map(len)>0]\n",
    "tweets_slctd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In `fraudulent_emails.txt`, identify the **list of email addresses** for your security administrator to blacklist from your company's email servers.\n",
    "\n",
    "* Not all emails are malicious! Provide only the list of email addresses from where the email originates from. **Hint**: identify the pattern in the emails that tells you the source of the email.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' markedeozo@freesurf.fr', ' elosub@freesurf.fr']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'(?<=From:)\\s*\\w+@\\w+\\.\\w{2}', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In `fraudulent_emails.txt`, identify any IP addresses that should be blacklisted\n",
    "\n",
    "An IPv4 address goes from **1.1.1.1 to 255.255.255.255**. For now, just worry about the number of digits, not whether the value in the address is above `255`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['902.213.181.', '677.213.181.', '196.207.0.', '62.56.147.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The word \"AT&T\" is not spelled correctly in the `tweets_df` dataset. Correct the misspelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      []\n",
       "1      []\n",
       "2      []\n",
       "3      []\n",
       "4      []\n",
       "5      []\n",
       "6      []\n",
       "7      []\n",
       "8      []\n",
       "9      []\n",
       "10     []\n",
       "11     []\n",
       "12     []\n",
       "13     []\n",
       "14     []\n",
       "15     []\n",
       "16     []\n",
       "17     []\n",
       "18     []\n",
       "19     []\n",
       "20     []\n",
       "21     []\n",
       "22     []\n",
       "23     []\n",
       "24     []\n",
       "25     []\n",
       "26     []\n",
       "27     []\n",
       "28     []\n",
       "29     []\n",
       "       ..\n",
       "468    []\n",
       "469    []\n",
       "470    []\n",
       "471    []\n",
       "472    []\n",
       "473    []\n",
       "474    []\n",
       "475    []\n",
       "476    []\n",
       "477    []\n",
       "478    []\n",
       "479    []\n",
       "480    []\n",
       "481    []\n",
       "482    []\n",
       "483    []\n",
       "484    []\n",
       "485    []\n",
       "486    []\n",
       "487    []\n",
       "488    []\n",
       "489    []\n",
       "490    []\n",
       "491    []\n",
       "492    []\n",
       "493    []\n",
       "494    []\n",
       "495    []\n",
       "496    []\n",
       "497    []\n",
       "Name: tweet, Length: 498, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['tweet'].str.findall(r'AT&amp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
       "1      Reading my kindle2...  Love it... Lee childs i...\n",
       "2      Ok, first assesment of the #kindle2 ...it fuck...\n",
       "3      @kenburbary You'll love your Kindle2. I've had...\n",
       "4      @mikefish  Fair enough. But i have the Kindle2...\n",
       "5      @richardebaker no. it is too big. I'm quite ha...\n",
       "6      Fuck this economy. I hate aig and their non lo...\n",
       "7                          Jquery is my new best friend.\n",
       "8                                          Loves twitter\n",
       "9      how can you not love Obama? he makes jokes abo...\n",
       "10     Check this video out -- President Obama at the...\n",
       "11     @Karoli I firmly believe that Obama/Pelosi hav...\n",
       "12     House Correspondents dinner was last night who...\n",
       "13     Watchin Espn..Jus seen this new Nike Commerica...\n",
       "14     dear nike, stop with the flywire. that shit is...\n",
       "15     #lebron best athlete of our generation, if not...\n",
       "16     I was talking to this guy last night and he wa...\n",
       "17                    i love lebron. http://bit.ly/PdHur\n",
       "18     @ludajuice Lebron is a Beast, but I'm still ch...\n",
       "19                                                #NAME?\n",
       "20     @sketchbug Lebron is a hometown hero to me, lo...\n",
       "21           lebron and zydrunas are such an awesome duo\n",
       "22                                                #NAME?\n",
       "23     downloading apps for my iphone! So much fun :-...\n",
       "24     good news, just had a call from the Visa offic...\n",
       "25     http://twurl.nl/epkr4b - awesome come back fro...\n",
       "26     In montreal for a long weekend of R&amp;R. Muc...\n",
       "27     Booz Allen Hamilton has a bad ass homegrown so...\n",
       "28     [#MLUC09] Customer Innovation Award Winner: Bo...\n",
       "29     @SoChi2 I current use the Nikon D90 and love i...\n",
       "                             ...                        \n",
       "468    Man I kinda dislike Apple right now. Case in p...\n",
       "469    @cwong08 I have a Kindle2 (&amp; Sony PRS-500)...\n",
       "470    The #Kindle2 seems the best eReader, but will ...\n",
       "471    I have a google addiction. Thank you for point...\n",
       "472                                               #NAME?\n",
       "473     Off to the bank to get my new visa platinum card\n",
       "474    dearest @google, you rich bastards! the VISA c...\n",
       "475    has a date with bobby flay and gut fieri from ...\n",
       "476    Excited about seeing Bobby Flay and Guy Fieri ...\n",
       "477    Gonna go see Bobby Flay 2moro at Shoreline. Ea...\n",
       "478    can't wait for the great american food and mus...\n",
       "479    My dad was in NY for a day, we ate at MESA gri...\n",
       "480                        Fighting with LaTex. Again...\n",
       "481    @Iheartseverus we love you too and don't want ...\n",
       "482    7 hours. 7 hours of inkscape crashing, normall...\n",
       "483    How to Track Iran with Social Media: http://bi...\n",
       "484    Shit's hitting the fan in Iran...craziness ind...\n",
       "485    Monday already. Iran may implode. Kitchen is a...\n",
       "486    Twitter Stock buzz: $AAPL $ES_F $SPY $SPX $PAL...\n",
       "487    getting ready to test out some burger receipes...\n",
       "488                                               #NAME?\n",
       "489    i lam so in love with Bobby Flay... he is my f...\n",
       "490    I just created my first LaTeX file from scratc...\n",
       "491    using Linux and loving it - so much nicer than...\n",
       "492    After using LaTeX a lot, any other typeset mat...\n",
       "493    Ask Programming: LaTeX or InDesign?: submitted...\n",
       "494    On that note, I hate Word. I hate Pages. I hat...\n",
       "495    Ahhh... back in a *real* text editing environm...\n",
       "496    Trouble in Iran, I see. Hmm. Iran. Iran so far...\n",
       "497    Reading the tweets coming out of Iran... The w...\n",
       "Name: tweet, Length: 498, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['tweet'].apply(lambda t: re.sub(r'\\b[aA][tT]{2}\\w+','AT&T',t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing hashtags and mentions, which topic has the highest average tweet character count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes</th>\n",
       "      <th>date</th>\n",
       "      <th>topic</th>\n",
       "      <th>handle</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>num</th>\n",
       "      <th>mention</th>\n",
       "      <th>child</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2</td>\n",
       "      <td>Tue Jun 02 03:27:48 UTC 2009</td>\n",
       "      <td>gm</td>\n",
       "      <td>hammerauto</td>\n",
       "      <td>RT @LATimesautos is now the time to buy a GM c...</td>\n",
       "      <td>69</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@LATimesautos]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>4</td>\n",
       "      <td>Tue Jun 02 03:29:53 UTC 2009</td>\n",
       "      <td>dentist</td>\n",
       "      <td>sardonnica</td>\n",
       "      <td>My wrist still hurts. I have to get it looked ...</td>\n",
       "      <td>139</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>4</td>\n",
       "      <td>Sun Jun 07 03:28:08 UTC 2009</td>\n",
       "      <td>warren buffet</td>\n",
       "      <td>Alfred04654</td>\n",
       "      <td>RT @blknprecious1: RT GREAT @dbroos \"Someone's...</td>\n",
       "      <td>137</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@blknprecious1, @dbroos]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Jun 08 19:59:23 UTC 2009</td>\n",
       "      <td>at&amp;t</td>\n",
       "      <td>mikeyil</td>\n",
       "      <td>@broskiii OH SNAP YOU WORK AT AT&amp;amp;T DON'T YOU</td>\n",
       "      <td>48</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@broskiii]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon Jun 08 19:59:26 UTC 2009</td>\n",
       "      <td>at&amp;t</td>\n",
       "      <td>nakiasmile</td>\n",
       "      <td>@Mbjthegreat i really dont want AT&amp;amp;T phone...</td>\n",
       "      <td>98</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@Mbjthegreat]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     likes                          date          topic       handle  \\\n",
       "160      2  Tue Jun 02 03:27:48 UTC 2009             gm   hammerauto   \n",
       "162      4  Tue Jun 02 03:29:53 UTC 2009        dentist   sardonnica   \n",
       "203      4  Sun Jun 07 03:28:08 UTC 2009  warren buffet  Alfred04654   \n",
       "224      0  Mon Jun 08 19:59:23 UTC 2009           at&t      mikeyil   \n",
       "225      0  Mon Jun 08 19:59:26 UTC 2009           at&t   nakiasmile   \n",
       "\n",
       "                                                 tweet  length num  \\\n",
       "160  RT @LATimesautos is now the time to buy a GM c...      69  []   \n",
       "162  My wrist still hurts. I have to get it looked ...     139  []   \n",
       "203  RT @blknprecious1: RT GREAT @dbroos \"Someone's...     137  []   \n",
       "224   @broskiii OH SNAP YOU WORK AT AT&amp;T DON'T YOU      48  []   \n",
       "225  @Mbjthegreat i really dont want AT&amp;T phone...      98  []   \n",
       "\n",
       "                       mention child hashtags  \n",
       "160            [@LATimesautos]    []       []  \n",
       "162                         []    []       []  \n",
       "203  [@blknprecious1, @dbroos]    []       []  \n",
       "224                [@broskiii]    []       []  \n",
       "225             [@Mbjthegreat]    []       []  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, we'll find the rows that contain AT&T to see what the pattern looks like\n",
    "tweets_df[tweets_df[\"tweet\"].str.contains(r'(AT)')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[\"tweet\"].str.replace(r'AT&amp;', 'AT&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
